{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14c6a47",
   "metadata": {},
   "source": [
    "# Sentiment Classification\n",
    "The Task of Sentiment Classification can be Broadly divided into two parts:\n",
    "1. Natural Language Processing\n",
    "2. Sentiment Classification\n",
    "    - Naive Bayes Classifier\n",
    "    - Support Vector Machines\n",
    "    - RoBERTa (AutoEncoder Transformers)\n",
    "\n",
    "So let us first import all the pre-requisites before hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daadb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U spacy\n",
    "%pip install spacy[transformer]\n",
    "%pip install pandas-profiling[notebook]\n",
    "%pip install -U scikit-learn\n",
    "%pip install -U matplotlib\n",
    "%pip install -U pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee6bde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ToDO \n",
    " prep all the packages for nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b46280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python -m spacy download en_core_web_sm\n",
    "%python -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d49a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_trainDF=pd.read_csv('train.csv')\n",
    "raw_testDF=pd.read_csv('test.csv')\n",
    "raw_trainDF.head()\n",
    "#prints the first five entries of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF=raw_trainDF\n",
    "testDF=raw_testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas profiling report\n",
    "from pandas_profiling import ProfileReport\n",
    "p_train=ProfileReport(raw_trainDF, title=\"Train profile report\")\n",
    "p_train.to_file(output_file=\"TrainReport.html\")\n",
    "p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87140a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test=ProfileReport(raw_testDF, title=\"Test profile Report\")\n",
    "p_test.to_file(output_file=\"TestReport.html\")\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed10c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization For the EDA\n",
    "#Train Set\n",
    "sent=trainDF.groupby(by=['ReviewAt']).count()['Sentiment']\n",
    "time=trainDF.ReviewAt.unique()\n",
    "\n",
    "fig=plt.figure(figsize=(15,7))\n",
    "plt.xticks(rotation=70)\n",
    "plt.bar(time, sent)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of Reviews by time (Train)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0808f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set\n",
    "sent=testDF.groupby(by=['ReviewAt']).count()['Sentiment']\n",
    "time=testDF.ReviewAt.unique()\n",
    "\n",
    "fig=plt.figure(figsize=(15,7))\n",
    "plt.xticks(rotation=70)\n",
    "plt.bar(time, sent)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of Reviews by time (Test)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82391a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing of Sentiment Category\n",
    "sent_names=list(set(trainDF.Sentiment.values))\n",
    "train_sent_cats=[]\n",
    "test_sent_cats=[]\n",
    "\n",
    "for name in sent_names:\n",
    "    train_sent_cats.append(trainDF.Sentiment.value_counts()[name])\n",
    "    test_sent_cats.append(testDF.Sentiment.value_counts()[name])\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].pie(train_sent_cats, labels=sent_names, autopct='%1.1f%%')\n",
    "ax[0].set_title(\"Sentiment Categories in Train Data\")\n",
    "ax[1].pie(test_sent_cats, labels=sent_names, autopct='%1.1f%%')\n",
    "ax[1].set_title(\"Sentiment Categories in Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde56a1",
   "metadata": {},
   "source": [
    "# Resplitting part\n",
    "Boring as repeat splitting and merging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285aed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=trainDF.OriginalReview[1]\n",
    "print(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the english library from SpaCy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#nlp() to all values\n",
    "testDF.nlp=testDF.OriginalTweet.apply(lambda x: nlp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b757cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.np=testDF.nlp.apply(lambda x: [chunk.text for chunk in x.noun_chunk])\n",
    "testDF.vb=testDF.nlp.apply(lambda x: [token.lemma_ for token in x if token.pos_ ==\"VERB\"])\n",
    "print(testDF.np.head())\n",
    "print(testDF.vb.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(sample_text)\n",
    "\n",
    "#Analyze syntax\n",
    "\n",
    "print(\"Noun phrase: \", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_==\"VERB\"])\n",
    "\n",
    "#Finding Named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07e411",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install contextualSpellCheck\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f1628",
   "metadata": {},
   "source": [
    "# Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82892d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label Encoder for classes in Sentiment\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "trainDF.encoded_sentiment=encoder.fit_transform(trainDF.Sentiment)\n",
    "trainDF.encoded_sentiment\n",
    "\n",
    "encoder=LabelEncoder()\n",
    "testDF.encoded_sentiment=encoder.fit_transform(testDF.Sentiment)\n",
    "testDF.encoded_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spelling correction\n",
    "import contextualSpellCheck\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#Add Contextual spellchecker to the pipeline\n",
    "\n",
    "nlp.add_pipe(\"contextual spellchecker\", config={\"max_edit_dist\": 5})\n",
    "\n",
    "doc=nlp(sample_text)\n",
    "\n",
    "print(doc._.outcome_spellCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Add contextual Spell check to pipeline\n",
    "nlp.add_pipe(\"Contextual Spellchecker\", config={\"max_edit_dist\": 5})\n",
    "\n",
    "#Create list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "#Create list of stopwords from spaCy\n",
    "stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Remove URLs\n",
    "def remove_urls(text):\n",
    "    text=re.sub(r\"\\S*https?:\\S&\",\"\",text,flags=re.MULTILINE)\n",
    "    return text\n",
    "#create tokenizer\n",
    "def spacy_tokenizer(sentence):\n",
    "    #init token obj\n",
    "    tokens=nlp(sentence)\n",
    "    #lemmetizer\n",
    "    tokens=[word.lemma_.lower().strip() if word.lemma_ !=\"PROPN\" else word.lower_ for word in tokens]\n",
    "    #Remove Stopwords\n",
    "    tokens=[word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "\n",
    "    #Remove Links\n",
    "    tokens = [remove_urls(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "spacy_tokenizer(sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908d76f",
   "metadata": {},
   "source": [
    "# (BoW) Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vector=CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949335e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "#custom class\n",
    "class predictors(TransformerMixin):\n",
    "    def clean_text(text):\n",
    "        return text.strip().lower()\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        # text Cleaning\n",
    "        return [clean_text(text) for text in X]\n",
    "    \n",
    "    def fit(self, Xm y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    bow_vector=CountVectorizer(tokenizer=spacy_tokenizer,ngram_range=(1,1))\n",
    "\n",
    "    #Multinomial Naive Bayes Classifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    classfier=MultinomialNB()\n",
    "\n",
    "    #pipeline\n",
    "    pipe=Pipeline([(\"cleaner\", predictors()),\n",
    "                   ('vectorizer', bow_vector),\n",
    "                   ('classfier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6089b",
   "metadata": {},
   "source": [
    "# Statistical Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=trainDF.OriginalTweet\n",
    "X_test=testDF.OriginalTweet\n",
    "y_train=trainDF.encoded_sentiment\n",
    "y_test=testDF.encoded_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier=MultinomialNB()\n",
    "\n",
    "pipeNB=Pipeline([(\"cleaner\",predictors()),\n",
    "                 (\"vectorizer\",bow_vector),\n",
    "                 (\"classifier\",classifier)])\n",
    "\n",
    "pipeNB.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Classfier\n",
    "from sklearn.svm import SVC\n",
    "classifier_svm=SVC()\n",
    "\n",
    "pipeSVM=Pipeline([(\"cleaner\",predictors()),\n",
    "                   (\"vectorizer\", bow_vector),\n",
    "                   (\"classifier\",classifier)])\n",
    "\n",
    "#model Generation\n",
    "pipeSVM.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6afe0b",
   "metadata": {},
   "source": [
    "# Time For Neural Network Shenanigans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd542ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing data\n",
    "def remove_url(text):\n",
    "    parsed_text=re.sub(r\"\\S*https?:\\S*\",\"\",text,flags=re.MULTILINE)\n",
    "    return parsed_text\n",
    "\n",
    "def preprocess(df, embed):\n",
    "    df.OriginalTweet=df.OriginalTweet.apply(remove_url)\n",
    "    data=tuple(zip(df.OriginalTweet.tolist(), df.Sentiment.tolist()))\n",
    "\n",
    "    nlp=spacy.load(embed)\n",
    "    print(data[0])\n",
    "\n",
    "    docs=[]\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total=len(data)):\n",
    "        if label=='Positive':\n",
    "            doc.cats['positive']=1\n",
    "            doc.cats['negative']=0\n",
    "            doc.cats['neutral']=0\n",
    "        elif label=='Neutral':\n",
    "            doc.cats['positive']=0\n",
    "            doc.cats['negative']=0\n",
    "            doc.cats['neutral']=1\n",
    "        else:\n",
    "            doc.cats['positive']=0\n",
    "            doc.cats['negative']=1\n",
    "            doc.cats['neutral']=0\n",
    "\n",
    "        docs.append(doc)\n",
    "    return df, docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbf397",
   "metadata": {},
   "source": [
    "# Spacy Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init from base_config\n",
    "%python -m spacy init fill-config ../config/base_config.cfg ../config/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd19d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python -m spacy debug data ../config/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25743cdd",
   "metadata": {},
   "source": [
    "Convert the dataframes to spacy files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess df for train data\n",
    "trainData,trainDocs=preprocess(trainDF,\"en_core_web_sm\")\n",
    "#save to disc\n",
    "doc_bin=DocBin(docs=trainDocs)\n",
    "doc_bin.to_disk(\"/spacy_data/textcat_train.spacy\")\n",
    "\n",
    "\n",
    "#preprocess df for test data\n",
    "testData,testDocs=preprocess(testDF,\"en_core_web_sm\")\n",
    "#save to disc\n",
    "doc_bin=DocBin(docs=testDocs)\n",
    "doc_bin.to_disk(\"/spacy_data/textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd29de9",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "we'll see about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb80941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the entities in the train and test docs\n",
    "train_loc=\"/spacy_data/textcat_train.spacy\"\n",
    "dev_loc=\"/spacy_data/textcat_train.spacy\"\n",
    "\n",
    "#Load library and train data\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc_bin=DocBin().from_disk(train_loc)\n",
    "docs=list(doc_bin.get_docs(nlp.vocab))\n",
    "entities=0\n",
    "\n",
    "#iterate through the docs\n",
    "for doc in docs:\n",
    "    entities+=len(doc.ents)\n",
    "print(f\"TRAIN docs: {len(docs)} with {entities} entities\")\n",
    "\n",
    "#Load Library and test data\n",
    "doc_bin=DocBin().from_disk(dev_loc)\n",
    "docs=list(doc_bin.get_docs(nlp.vocab))\n",
    "entities=0\n",
    "\n",
    "#iterate over the docs\n",
    "for doc in docs:\n",
    "    entities+=len(doc.ents)\n",
    "print(f\"DEV docs: {len(docs)} with {entities} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "%python -m spacy train ../config/config.cfg --verbose --output ../data/textcat_output --path.train ../data/spacy_data/textcat_train.spacy --paths.dev ../data/spacy_data/textcat_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca64509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick the best epoch model\n",
    "#verify model\n",
    "nlp_model=spacy.load(\"../data/textcat_output/model-best\")\n",
    "test_text=testData.OriginalTweet.tolist()\n",
    "test_cats=testData.Sentiment.tolist()\n",
    "doc_test=nlp_model(test_text[20])\n",
    "print(\"Text: \"+ test_text[20])\n",
    "print(\"Orig Cat: \"+ test_cats[20])\n",
    "print(\"Predicted Cats: \")\n",
    "print(doc_test.cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7011d",
   "metadata": {},
   "source": [
    "# Pre-Trained Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train and test dataframes to .spacy files for training\n",
    "\n",
    "#Preprocess the dataframes for train data\n",
    "doc_bin=DocBin(docs=trainDocs)\n",
    "doc_bin.to_disk(\"/spacy_data/text_roberta_train.spacy\")\n",
    "\n",
    "#Preprocess the dataframes for test data\n",
    "test_data_roberta, testDocs=preprocess(testDF,\"en_core_web_trf\")\n",
    "\n",
    "#Save data and docs in a binary file to disc\n",
    "doc_bin=DocBin(docs=testDocs)\n",
    "doc_bin.to_disk(\"data/spacy_data/textcat_roberta_valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46287463",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a842f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python -m spacy train ../config/config.cfg --verbose --output ../data/textcat_roberta_output --paths.train ../data/spacy_data/textcat_roberta_train.spacy --paths.dev ../data/spacy_data/textcat_roberta_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification\n",
    "nlp_model=spacy.load(\"../data/textcat_roberta_output/model-best\")\n",
    "test_text=test_data_roberta.OriginalTweet.tolist()\n",
    "test_cats=test_data_roberta.Sentiment.tolist()\n",
    "doc_test=nlp_model(test_text[20])\n",
    "print(\"Text: \"+ test_text[20])\n",
    "print(\"Og Cat: \"+ test_text[20])\n",
    "print(\"Predicted Cats: \")\n",
    "print(doc_test.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf104cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the train and test dataframes to .spacy files for training\n",
    "\n",
    "#preprocess df for valid data\n",
    "valid_data, valid_docs=preprocess(validDF,\"en_core_web_sm\")\n",
    "valid_data_roberta, valid_docs=preprocess(validDF,\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify for english model\n",
    "nlp_model=spacy.laod(\"../data/textcat_output/model-best\")\n",
    "valid_text=valid_data.OriginalTweet.tolist()\n",
    "valid_cats=valid_data.Sentiment.tolist()\n",
    "doc_valid=nlp_model(valid_text[50])\n",
    "print(\"Text: \"+ valid_text[50])\n",
    "print(\"OG cat: \"+ valid_cats[50])\n",
    "\n",
    "print(\"Predicted Cats:\")\n",
    "print(doc_valid.cats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7638cc",
   "metadata": {},
   "source": [
    "Done! This is to verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccacb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model_bert = spacy.load(\"../data/textcat_roberta_output/model-best\")\n",
    "doc_valid_bert = nlp_model_bert(valid_text[50])\n",
    "print(\"Text: \"+ valid_text[50])\n",
    "print(\"Orig Cat: \"+ valid_cats[50])\n",
    "print(\" Predicted Cats:\") \n",
    "print(doc_valid_bert.cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e4b47",
   "metadata": {},
   "source": [
    "# Generating a Report and analyzing/ Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict with a test dataset\n",
    "predicted = pipeNB.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Naive Bayes Model:\\n\")\n",
    "print(classification_report(y_test, predicted, target_names = ['Negative', 'Neutral', 'Positive']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efddfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "# Predicting with a test dataset\n",
    "predicted = pipeSVM.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Support Vector Machine:\\n\")\n",
    "print(classification_report(y_test, predicted, target_names = ['Negative', 'Neutral', 'Positive']))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
